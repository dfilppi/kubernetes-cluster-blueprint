###########################################################
# This Blueprint installs Kubernetes on Openstack
###########################################################

tosca_definitions_version: cloudify_dsl_1_2

imports:  
  - http://getcloudify.org/spec/cloudify/3.3.1/types.yaml
  - http://getcloudify.org/spec/diamond-plugin/1.3.1/plugin.yaml
  - http://getcloudify.org/spec/openstack-plugin/1.3.1/plugin.yaml
  - types/scale.yaml

inputs:

  image:
    description: >
      Image to be used when launching agent VM's

  flavor:
    description: >
      Flavor of the agent VM's

  agent_user:
    description: >
      User for connecting to agent VM's

  master_port:
    description: >
      The port that the master API will respond to.
    default: 8080

  etcd_version:
    description: >
      The etcd version.
    default: '2.0.12'

  flannel_version:
    description: >
      The flannel version
    default: '0.5.3'

  kubernetes_version:
    description: >
      The kubernetes version.
    default: 'v1.0.1'

node_templates:

  kubernetes_master_host:
    type: cloudify.openstack.nodes.Server
    instances:
      deploy: 1
    properties:
      cloudify_agent:
        user: ubuntu
      image: {get_input: image}
      flavor: {get_input: flavor}
      server:
        userdata: |
          #!/bin/bash
          if [ ! -f /usr/bin/docker ]; then
            sudo service ssh stop
            curl -o install.sh -sSL https://get.docker.com/
            sudo sh install.sh
            sudo groupadd docker
            sudo gpasswd -a ubuntu docker
            sudo service docker restart
            sudo service ssh start
          fi
    relationships:
      - target: kubernetes_security_group
        type: cloudify.openstack.server_connected_to_security_group
      - type: cloudify.openstack.server_connected_to_floating_ip
        target: kubernetes_master_ip

  kubernetes_master_ip:
    type: cloudify.openstack.nodes.FloatingIP

  kubernetes_security_group:
    type: cloudify.openstack.nodes.SecurityGroup
    properties:
      security_group:
        name: kubernetes_security_group
        description: kubernetes master security group
      rules:
      - remote_ip_prefix: 0.0.0.0/0  # for remote install
        port: 22
      - remote_ip_prefix: 0.0.0.0/0
        port: { get_input: master_port }
      - remote_ip_prefix: 0.0.0.0/0  # for minions
        port: 4001
      - remote_ip_prefix: 0.0.0.0/0  # for service
        port: 30000

  docker_kubernetes_master:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        start: scripts/docker/bootstrap.py
    relationships:
      - type: cloudify.relationships.contained_in
        target: kubernetes_master_host

  etcd_kubernetes_master:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        create:
          implementation: scripts/etcd/create.py
          inputs:
            etcd_version: { get_input: etcd_version }
    relationships:
      - type: cloudify.relationships.contained_in
        target: kubernetes_master_host
      - type: cloudify.relationships.depends_on
        target: docker_kubernetes_master

  flannel_kubernetes_master:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        create:
          implementation: scripts/flannel/master/create.py
          inputs:
            flannel_version: { get_input: flannel_version }
        configure:
          implementation: scripts/flannel/master/configure.py
    relationships:
      - type: cloudify.relationships.contained_in
        target: kubernetes_master_host
      - type: cloudify.relationships.depends_on
        target: etcd_kubernetes_master

  kubernetes_master:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        configure:
          implementation: scripts/kubernetes/master/create.py
          inputs:
            master_port: { get_input: master_port }
        start:
          implementation: scripts/kubernetes/master/start.py
          inputs:
            master_port: { get_input: master_port }
    relationships:
      - type: cloudify.relationships.contained_in
        target: kubernetes_master_host
      - type: cloudify.relationships.depends_on
        target: flannel_kubernetes_master

  kubernetes_master_proxy:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        create:
          implementation: scripts/kubernetes/master/run_proxy.py
          inputs:
            master_port: { get_input: master_port }
    relationships:
      - type: cloudify.relationships.contained_in
        target: kubernetes_master_host
      - type: cloudify.relationships.depends_on
        target: kubernetes_master

  kubectl:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        create:
          implementation: scripts/kubernetes/kubectl.py
    relationships:
      - type: cloudify.relationships.contained_in
        target: kubernetes_master_host
      - type: cloudify.relationships.depends_on
        target: kubernetes_master_proxy

  kubernetes_node_host:
    type: cloudify.openstack.nodes.Server
    instances:
      deploy: 2
    properties:
      cloudify_agent:
        user: ubuntu
      image: {get_input: image}
      flavor: {get_input: flavor}
      server:
        userdata: |
          #!/bin/bash
          service ssh stop
          curl -o install.sh -sSL https://get.docker.com/
          sh install.sh
          groupadd docker
          gpasswd -a ubuntu docker
          service docker stop
          service ssh start
    relationships:
      - target: kubernetes_security_group
        type: cloudify.openstack.server_connected_to_security_group
    interfaces:

      ###########################################################
      # We are infact telling cloudify to install a diamond
      # monitoring agent on the server.
      #
      # (see https://github.com/BrightcoveOS/Diamond)
      ###########################################################

      cloudify.interfaces.monitoring_agent:
          install:
            implementation: diamond.diamond_agent.tasks.install
            inputs:
              diamond_config:
                interval: 1
          start: diamond.diamond_agent.tasks.start
          stop: diamond.diamond_agent.tasks.stop
          uninstall: diamond.diamond_agent.tasks.uninstall

      ###########################################################
      # Adding some collectors. These collectors are necessary
      # for the Cloudify UI to display the deafult metrics.
      ###########################################################

      cloudify.interfaces.monitoring:
          start:
            implementation: diamond.diamond_agent.tasks.add_collectors
            inputs:
              collectors_config:
                CPUCollector: {}
                MemoryCollector: {}
                LoadAverageCollector: {}
                DiskUsageCollector:
                  config:
                    devices: x?vd[a-z]+[0-9]*$
                NetworkCollector: {}
                ProcessResourcesCollector:
                  config:
                    enabled: true
                    unit: B
                    measure_collector_time: true
                    cpu_interval: 0.5
                    process:
                      hyperkube:
                        name: hyperkube

  docker_kubernetes_node:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        start: scripts/docker/bootstrap.py
    relationships:
      - type: cloudify.relationships.contained_in
        target: kubernetes_node_host

  flannel_kubernetes_node:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        create:
          implementation: scripts/flannel/node/create.py
          inputs:
            master_ip: { get_attribute: [ kubernetes_master_host, ip ] }
            flannel_version: { get_input: flannel_version }
    relationships:
      - type: cloudify.relationships.contained_in
        target: kubernetes_node_host
      - type: cloudify.relationships.depends_on
        target: docker_kubernetes_node
      - type: cloudify.relationships.connected_to
        target: kubernetes_master_proxy

  kubernetes_node:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        start:
          implementation: scripts/kubernetes/node/start.py
          inputs:
            master_ip: { get_attribute: [ kubernetes_master_host, ip ] }
            master_port: { get_input: master_port }
    relationships:
      - type: cloudify.relationships.connected_to 
        target: kubernetes_master
      - type: cloudify.relationships.contained_in
        target: kubernetes_node_host
      - type: cloudify.relationships.depends_on
        target: flannel_kubernetes_node
      - type: cloudify.relationships.depends_on
        target: kubernetes_master_proxy

  kubernetes_node_proxy:
    type: cloudify.nodes.Root
    interfaces:
      cloudify.interfaces.lifecycle:
        create:
          implementation: scripts/kubernetes/node/proxy.py
          inputs:
            master_ip: { get_attribute: [ kubernetes_master_host, ip ] }
            master_port: { get_input: master_port }
    relationships:
      - type: cloudify.relationships.contained_in
        target: kubernetes_node_host
      - type: cloudify.relationships.depends_on
        target: kubernetes_node

groups:

  scale_up_group:
    members: [kubernetes_node_host]
    policies:
      auto_scale_up:
        type: scale_policy_type
        properties:
          policy_operates_on_group: true
          scale_limit: 6
          scale_direction: '<'
          scale_threshold: 3
          service_selector: .*kubernetes_node_host.*.process.hyperkube.cpu.percent
          cooldown_time: 60
        triggers:
          execute_scale_workflow:
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              workflow: scale
              workflow_parameters:
                delta: 1
                node_id: kubernetes_node_proxy
                scale_compute: true

  scale_down_group:
    members: [kubernetes_node_host]
    policies:
      auto_scale_down:
        type: scale_policy_type
        properties:
          scale_limit: 2
          scale_direction: '>'
          scale_threshold: 1
          service_selector: .*kubernetes_node_host.*.process.hyperkube.cpu.percent
          cooldown_time: 60
          moving_window_size: 200
        triggers:
          execute_scale_workflow:
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              workflow: scale
              workflow_parameters:
                delta: -1
                node_id: kubernetes_node_proxy
                scale_compute: true

  heal_group:
    members: [kubernetes_node_host]
    policies:
      simple_autoheal_policy:
        type: cloudify.policies.types.host_failure
        properties:
          service:
            - .*kubernetes_node_host.*.cpu.total.system
            - .*kubernetes_node_host.*.process.hyperkube.cpu.percent
          interval_between_workflows: 60
        triggers:
          auto_heal_trigger:
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              workflow: heal
              workflow_parameters:
                node_instance_id: { 'get_property': [ SELF, node_id ] }
                diagnose_value: { 'get_property': [ SELF, diagnose ] }

outputs:
  kubernetes_info:
    description: Kuberenetes master info
    value:
      url: {concat: ["http://",{ get_attribute: [ kubernetes_master_ip, floating_ip_address ]},":", { get_input: master_port } ] }
